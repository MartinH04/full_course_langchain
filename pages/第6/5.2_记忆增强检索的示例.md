## 记忆增强检索的示例

本节我们将深入探讨如何通过集成记忆组件，增强我们的数据连接模块LEDVR工作流中的数据检索能力，从而提升QA问答应用的回答质量。我们将以《链》章节中的代码案例为基础，介绍如何在已有的检索器基础上，添加记忆组件，使ConversationalRetrievalChain链组件具有“记忆”能力。

在此基础上，我们的案例应用将得以进一步提升。以LUA语言开发者在线文档问答为例，通过增强后的链组件，我们的程序不仅能在 http://developers.mini1.cn/wiki/luawh.html 中找到相关的文档内容，准确回答如“LUA宿主语言是什么”等问题，更能够记住用户在对话中的相关信息，如用户的名字，从而对更多复杂的问题提供更准确的答复。例如，当用户在问完几轮关于LUA语言编程的问题之后，如果他们再次提到他们的名字或引用他们之前提到的信息，增强后的ConversationalRetrievalChain链组件能够记住并理解这些信息，提供更加精确和个性化的回答。

希望读者能够理解并掌握如何将记忆组件与链组件集成，从而实现数据检索能力的增强，提升QA问答应用的回答质量。同时，我们也将向读者展示如何通过增加记忆组件，使我们的程序更具人性化，能够更好地满足用户的需求。使用加载器开始，到创建向量存储库实例都与《链》章节一样。如果你已经很熟悉这段代码，可以直接跳过这部分。

首先，我们需要从网络加载文档。这可以通过使用WebBaseLoader来完成：

from langchain.document_loaders import WebBaseLoader
openai_api_key="填入你的密钥"
loader = WebBaseLoader("http://developers.mini1.cn/wiki/luawh.html")
data = loader.load()

你也可以选择其他的加载器和其他的文档资源。接下来，我们需要创建一个嵌入模型实例的包装器，这可以使用OpenAIEmbeddings完成：

from langchain.embeddings.openai import OpenAIEmbeddings
embedding = OpenAIEmbeddings(openai_api_key=openai_api_key)

然后，我们需要将文档切割成块，这可以通过使用RecursiveCharacterTextSplitter来完成：

from langchain.text_splitter import RecursiveCharacterTextSplitter
text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)
splits = text_splitter.split_documents(data)

接着，我们需要创建一个向量存储库，这里我们选择使用FAISS，现在我们有了一个向量存储库：

from langchain.vectorstores import FAISS
vectordb = FAISS.from_documents(documents=splits,embedding=embedding)

#### 记忆增强检索的代码实践

我们首先导入 ConversationBufferMemory 类，这是最常见的记忆组件，其工作原理非常简单：仅将所有聊天记录保存起来，而不使用任何算法进行截取或提炼压缩。在提示词模板中，我们可以看到所有的聊天记录。

from langchain.memory import ConversationBufferMemory
memory = ConversationBufferMemory(memory_key="chat_history",return_messages=True)

初始化记忆组件后，我们可以看到其内部的存储情况。由于我们刚刚进行了初始化，所以存储的聊天记录为空。然后，我们通过 add_user_message 方法添加一个人类的消息，向程序介绍我的名字。此时，我们再次查看记忆组件，就可以看到添加了一条 HumanMessage 消息。我们看看初始化后，记忆组件里保存着什么记录。因为我们刚刚初始化，所以是[]。

# 打印 memory.chat_memory.messages
[]

我们使用 add_user_message 添加一个人类的消息，向程序介绍我的名字。

memory.chat_memory.add_user_message("我是李特丽")

第一次打印 memory.chat_memory.messages 时为[], 当我推送一条自我介绍后，我们可以看到添加了一条 HumanMessage 消息。

[HumanMessage(content='我是李特丽', additional_kwargs={}, example=False)]

使用 memory 组件的 load_memory_variables 方法可以看保存在程序运行内存中的 memory 对象，主键是 chat_history，正如我们初始化 ConversationBufferMemory 设置的那样：memory_key="chat_history" 。

# 打印memory.load_memory_variables({})
{'chat_history': [HumanMessage(content='我是李特丽', additional_kwargs={}, example=False)]}

我们采用最直接的方式来演示记忆组件是如何与链组件共同工作的。从提示词模板出发，逐步增加组件的方式，展示这个工作机制。这一次我们使用的链组件是 load_qa_chain。这个链组件的优势在于专门用于QA问答，对于合并文档链组件不必掌握这些链的使用方法，只要在这个链上指定类型。导入 load_qa_chain 
、ChatOpenAI 和 PromptTemplate。提示词模板用于我们初始化 load_qa_chain 的时候，个性化配置提示词。


from langchain.chat_models import ChatOpenAI
from langchain.chains.question_answering import load_qa_chain
from langchain.prompts import PromptTemplate

# 这里需注意使用 Chat Model 类型的模型包装器，并且使用先进的模型型号。
llm = ChatOpenAI(openai_api_key=openai_api_key,temperature=0, model="gpt-3.5-turbo-0613")

使用向量存储库实例 的 similarity_search 方法，测试是否可以检索到与问题相关的文档了。我们可以打印 len(docs)， 看看这个问题搜索到了几个文档片段。检索到的相关文档内容，我们要输入到提示模板包装器中，这一步先测试向量存储库是否正常工作。

query = "LUA 的宿主语言是什么？"
docs = vectordb.similarity_search(query)
docs

打印 len(docs) 的长度是4，有4个相关文档片段被检索到。

4

创建提示词是最重要的环节，在创建的过程中我们可以理解为什么加入记忆后，因为有了“聊天备忘录” 记录的内容，让链组件有了 “记忆”。使用提示模板包装器，我们自定义一个提示词模板字符串。提示词内容四部分：一是对模型的指导词：“请你回答问题的时候，请依据文档内容和聊天记录回答，如果在其中找不到相关信息或者答案，请回答不知道。” ； 二是用问题检索到的相关文档内容：“文档内容是：{context}” ；三是记忆组件输出的记忆内容：“聊天记录是：{chat_history}” ；四是用户的输入： “Human: {human_input}”。

template = """你是说中文的chatbot.

请你回答问题的时候，请依据文档内容和聊天记录回答，如果在其中找不到相关信息或者答案，请回答不知道。

文档内容是：{context}

聊天记录是：{chat_history}
Human: {human_input}
Chatbot:"""

prompt = PromptTemplate(
    input_variables=["chat_history", "human_input", "context"], template=template
)

记忆组件除了指定记忆存储对象的键值，还要指定 input_key 。load_qa_chain 链组件在运行时，会解析 input_key ，将值对应到模板字符串的用户输入 human_input 占位符中。

memory = ConversationBufferMemory(memory_key="chat_history", input_key="human_input")
chain = load_qa_chain(
    llm=llm, chain_type="stuff", memory=memory, prompt=prompt
)

刚刚我们把记忆组件加入到 load_qa_chain 链组件中，这个链组件就有了记忆。我们向这个链组件发出第一个问题: “ LUA 的宿主语言是什么？”

query = "LUA 的宿主语言是什么？"
docs = vectordb.similarity_search(query)
chain({"input_documents": docs, "human_input": query}, return_only_outputs=True)

不出意外，运行链组件后，我们得到了正确的答案。这个答案正是来源于我们之前检索的四个文档片段。

{'output_text': 'Lua的宿主语言通常是C或C++。'}

接着我们可以相互之间来个自我介绍。

query = "我的名字是李特丽。你叫什么？"
docs = vectordb.similarity_search(query)
chain({"input_documents": docs, "human_input": query}, return_only_outputs=True)

大语言模型回答的是：“我是一个中文的chatbot。”

{'output_text': '我是一个中文的chatbot。'}

我们继续模拟一下正常的聊天应用程序，问一些别的问题，目的是测试一下，几个问题后，他是否还能记得我们的名字，如果他能记得，则证明他有了记忆。我们先问技术文档上的问题。

query = "LUA 的循环语句是什么？"
docs = vectordb.similarity_search(query)
chain({"input_documents": docs, "human_input": query}, return_only_outputs=True)

回答依然是准确的。

{'output_text': 'Lua的循环语句有while循环、for循环和repeat...until循环。'}

现在我们可以测试他是不是记得我们的名字了。

query = "我的名字是什么？"
docs = vectordb.similarity_search(query)
chain({"input_documents": docs, "human_input": query}, return_only_outputs=True)

他记得我们的名字了。

{'output_text': '你的名字是李特丽。'}

我们打印看看他记忆了什么内容：

print(chain.memory.buffer)

显然，他把我们之间的聊天记录都记录下来了。用户的问题和模型的回答。

Human: LUA 的宿主语言是什么？
AI: Lua的宿主语言通常是C或C++。
Human: 我的名字是李特丽。你叫什么？
AI: 我是一个中文的chatbot。
Human: LUA 的循环语句是什么？
AI: Lua的循环语句有while循环、for循环和repeat...until循环。
Human: 我的名字是什么？
AI: 你的名字是李特丽。

在这个代码实践中，我们将记忆组件与 load_qa_chain 链组件一起使用，让这个链组件具有记忆能力。我们通过问一系列问题，观察链组件的回答，从而验证其记忆能力。例如，当我们问了几个关于 LUA 语言的问题之后，再问它我们的名字，它还能正确回答，这就证明了它具有记忆能力。

需要注意的是，这些聊天记录并不会一直保存，它们只保留在运行程序的内存中，一旦程序停止运行，这些记录就会消失。这个示例代码的目的是解释如何让一个 QA 问答链具备“记忆”的能力，实现检索增强。如果没有记忆的能力，对于用户来说，这个程序就会看起来很木讷，因为凡是“关于人”的问题，它回答的都是“不知道”。

总的来说，通过以上的代码实践，我们了解了如何使用 ConversationBufferMemory 这个最基本的记忆组件，包括如何实例化记忆组件，如何使用其存储和读取聊天记录的功能，以及如何将其与其他组件（例如 load_qa_chain 链组件）组合使用，来增强程序的功能。






