
# 用 LangChain 自制 chatPDF

这个 ChatPDF 项目是引入外部数据集对大语言模型进行微调，以生成更准确的回答的程序。

试想你是一个航天飞机设计师，你需要了解最新的航空材料技术，你可以将这个需求输入到的模型中，模型就会根据最新的数据集给出准确的答案。

的界面呈现的是人类与文档问答的聊天，但实质上，仍然是在与大语言模型交流，只不过这个模型现在被赋予了接入外部数据集的能力。就像你在与一位熟悉你公司内部文档的同事交谈，尽管他可能并未参与过这些文档的编写，但他可以准确地回答你的问题。

大语言模型之前，不能像聊天一样与文档交流，只能依赖于搜索。例如, 你正在为一项重要的报告寻找资料，你必须知道你需要查找的关键词，然后在大量的信息中筛选出你需要的部分。而现在，可以通过聊天的方式，即使不知道具体的关键词，也可以让模型根据的问题告诉答案。就好像你在问一位专业的图书馆员，哪些书籍可以帮助你完成这份报告。

那为什么要引入文档的外部数据集呢？这是因为大语言模型的训练数据截止到 2021 年 9 月，之后产生的知识和信息并未被包含进去。就像的模型是一个生活在过去的时间旅行者，他只能告诉你他离开的那个时刻之前的所有信息，对之后的事情一无所知。

的模型训练数据不仅包含参数，也引入了外部数据集进行训练。就好像通过电话，向一个在远方的朋友讲述最新的新闻，这样他就可以了解到最新的事情。的大语言模型也一样，通过外部数据集的输入，可以使它了解到最新的信息，从而进行更好的检索增强和微调，适应的实时需求。

引入外部数据集还有一个重要的目的，那就是修复大语言模型的 "机器幻觉"，避免给出错误的回答。试想一下，如果你向一个只知道过去信息的人询问未来的趋势，他可能会基于过去的信息进行推断，但这样的答案未必正确。所以通过引入最新的数据，让的模型能够更准确地回答问题，避免因为信息过时产生的误导。

现在使用的数据文档形式包括 pdf、json、word、excel 等，这些都是获取实时知识和数据的途径。这类程序现在非常受欢迎，比如最著名的 chatpdf 和 chatdoc, 还有针对各种特定领域的程序，如针对法律文档的程序。就像你在阅读各种格式的书籍一样，不同的程序能够提供不同的知识和信息。



### 7.3.1 程序流程

的实现方式是利用 Langchain 已实现的向量存储、嵌入以及使用查询和检索相关的链，来获取外部数据集，处理文档，进行相关性检索后合并处理，置入大语言模型的提示模板中，实现与 PDF 文件交流的目的。

选定的文档是 Reid Hoffman 写的一本关于 GPT-4 和人工智能的书，将下载这本 PDF 并将其转化为可查询和交互的形式。

##### 加载文件且切分块

首先，要加载 PDF 文件，并将其分割成块。接下来，将创建一个向量存储，并对其进行查询。这个过程很像使用搜索引擎，在向量存储中查找相关内容，但与基于关键词的搜索不同，是基于语义的。

##### 嵌入和向量存储

为每个块创建嵌入，这个嵌入会是一个向量，代表该块中的所有信息。当需要查询某个信息时，会将问题嵌入为一个向量，然后发送给向量存储器。向量存储器会返回最相关的信息块，然后将其与问题一起传递给语言模型。

##### 语言模型 I/O

语言模型会阅读这些信息，并根据这些信息决定答案是什么。这种基于语义搜索的概念并不新，甚至使用大语言模型来查询文本的想法也已经存在很长时间了。

##### LangChain 创建链

需要做的关键步骤包括：加载文档，分割文档，创建嵌入，将嵌入放入向量存储。接下来，将使用 LangChain 创建的链，以便可以进行查询，查找向量存储中的信息，将其带入链中，然后与问题结合，最终得出一个答案。



### 7.3.2 环境和工具

##### 安装包
```
!pip -q install langchain openai tiktoken PyPDF2 faiss-cpu

```
##### 设置密钥
```

import os

os.environ["OPENAI_API_KEY"] = ""
```

##### 下载 PDF 文本 
```
# 可以复制到浏览器后，直接保存在本地电脑上。
!wget -q https://www.impromptubook.com/wp-content/uploads/2023/03/impromptu-rh.pdf
```

### 导入库和 PDF 加载器

首先，需要一个 PDF 阅读器。虽然这次只使用了一个基础的 PDF 阅读器，你也可以根据需要选择更合适的 PDF 阅读器。通过 PDF 阅读器将 PDF 文档读取成一个长字符串。这个过程可能会遇到一些格式问题，比如奇怪的空格等，但每个项目都有自己独特的数据处理方法。不论是使用基础的处理方式，还是使用诸如'unstructured'库、AWS 或 Google Cloud 的 API，都有可能。


先导入阅读器 `PdfReader`, 嵌入模型 `OpenAIEmbeddings`, 文档切分器 `CharacterTextSplitter`, 向量存储库 `FAISS`  ： 

```
from PyPDF2 import PdfReader
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores import FAISS 
```

加载之前准备好的 PDF 素材：

```
# 括号内填入当前电脑保存pdf文件的位置. 
doc_reader = PdfReader('/content/impromptu-rh.pdf')
```
打印 `doc_reader` 看看 :
```
<PyPDF2._reader.PdfReader at 0x7f119f57f640>
```
将 PDF 文档转化为可用的 `raw_text` 格式：

```
# read data from the file and put them into a variable called raw_text
raw_text = ''
for i, page in enumerate(doc_reader.pages):
    text = page.extract_text()
    if text:
        raw_text += text
```     
可以打印 `raw_text` 的结果字符串长度，看看是不是转换成功了：

```
len(raw_text) # 得到 356710 的结果
```


### 7.3.3 文本拆分

需要将获取到的长字符串拆分为适合分析的小段落。

的方法很简单，就是将这个长字符串按照字符数拆分。比如可以设定每 1000 个字符为一个块, `chunk_size = 1000`。


```

# Splitting up the text into smaller chunks for indexing
text_splitter = CharacterTextSplitter(        
    separator = "\n",
    chunk_size = 1000,
    chunk_overlap  = 200, #striding over the text
    length_function = len,
)
texts = text_splitter.split_text(raw_text)

```

总共切了 448 个块：

```
len(texts) # 448
``` 

*注意*： 在这个代码片段中，`chunk_overlap` 参数用于指定文本切分时的重叠量（overlap）。它表示在切分后生成的每个分块之间重叠的字符数。具体来说，这个参数表示每个分块的前后，两个分块之间会有多少个字符是重复的。举例来说 chunkA 和 chunkB, 他们有 200 个字符是重复的。

然后，采用滑动窗口的方法来拆分文本。即每个块之间会有部分字符重叠，比如在每 1000 个字符的块上，让前后两块有 200 个字符重叠。这样做的目的是避免关键信息被切分，而且即使有些信息出现在了多个块中，因为是在获取整体语义，所以这些重叠的块在语义上也会有所区别。

可以随机打印一块的内容：

```
texts[20]
```

输出是：

```
'million registered users. \nIn late January 2023, Microsoft1—which had invested $1 billion \nin OpenAI in 2019—announced that it would be investing $10 \nbillion more in the company. It soon unveiled a new version of \nits search engine Bing, with a variation of ChatGPT built into it.\n1 I sit on Microsoft’s Board of Directors. 10Impromptu: Amplifying Our Humanity Through AI\nBy the start of February 2023, OpenAI said ChatGPT had \none hundred million monthly active users, making it the fast-\nest-growing consumer internet app ever. Along with that \ntorrent of user interest, there were news stories of the new Bing \nchatbot functioning in sporadically unusual ways that were \nvery different from how ChatGPT had generally been engaging \nwith users—including showing “anger,” hurling insults, boast-\ning on its hacking abilities and capacity for revenge, and basi-\ncally acting as if it were auditioning for a future episode of Real \nHousewives: Black Mirror Edition .'
```



### 7.3.4  创建嵌入和检索

有了分好的小块文本，就可以为这些文本创建嵌入了。在这个步骤，使用了 OpenAI 的嵌入技术。

```
# Download embeddings from OpenAI
embeddings = OpenAIEmbeddings()
```
先把文本传给嵌入制造器，然后通过 FAISS 库创建向量存储本身。

```
docsearch = FAISS.from_texts(texts, embeddings)
```

至此，已经将原本的 PDF 文档转化为了可以进行机器学习的向量数据。

就是这么简单，接下来就可以向这个 PDF 问问题了。

### 相似度检索

现在，可以使用这些向量数据来进行搜索匹配了。以一个实际的查询为例：“GPT-4 如何改变社交媒体？”。

```
query = "GPT-4 如何改变了社交媒体?"
docs = docsearch.similarity_search(query) # 这是搜索匹配的文字结果数组。
```

把这个查询传给文档搜索系统，使用相似度匹配搜索 `similarity_search`，在向量数据中寻找与查询最匹配的文档。

的搜索结果将包括与查询最接近的四个文档，而这些文档都是通过的嵌入函数进行嵌入的。

```

len(docs)  # 结果为 4 代表有 4 处地方跟问题有关系

```

尝试打印第一个 `docs[0]`
在的搜索结果中，首位的文档中多次提到了“社交媒体”，看来的查询效果还是很好的。

```
Document(page_content='rected ways that tools like GPT-4 and DALL-E 2 enable.\nThis is a theme I’ve touched on throughout this travelog, but \nit’s especially relevant in this chapter. From its inception, social \nmedia worked to recast broadcast media’s monolithic and \npassive audiences as interactive, democratic communities, in \nwhich newly empowered participants could connect directly \nwith each other. They could project their own voices broadly, \nwith no editorial “gatekeeping” beyond a given platform’s terms \nof service.\nEven with the rise of recommendation algorithms, social media \nremains a medium where users have more chance to deter -\nmine their own pathways and experiences than they do in the \nworld of traditional media. It’s a medium where they’ve come \nto expect a certain level of autonomy, and typically they look for \nnew ways to expand it.\nSocial media content creators also wear a lot of hats, especially \nwhen starting out. A new YouTube creator is probably not only', metadata={})
```

这就是如何利用 OpenAI 技术处理 PDF 文档，将海量的信息提炼为可用的数据的全部步骤。是不是很简单，赶紧动手做起来吧~

现在只有一个 PDF 文档，实现代码也很简单，Langchain 给了很多组件，完成得很快。接下来，处理多文档的提问，现实是要获取到真实的信息，通过会跨越多个文档，才能提取有用的信息。比如读取金融研报，新闻综合报道等等。


### 7.3.5  进阶 Stuff 链

在上一节中，加载了一个 PDF 文档，转化格式，切分字符后，创建向量数据来进行搜索匹配获得了问题的答案。一旦有了已经处理好的文档，就可以开始构建一个简单的问答链。现在看看 如何使用 Langchain 构建问答链。 

在这个过程中，使用了 OpenAI 的模型，并选择了 Langchain 的现有文档处理链中 一种被称为 "stuff" 的链类型。在这种模式下，只是将所有内容都放在一个调用中，理想情况下，放入的内容应该少于 4000 个令牌。

除了 "stuff" 之外，Langchain 文档处理链还有 精化（Refine）、Map reduce 、重排（Map re-rank）。后面会再次用到。

```

from langchain.chains.question_answering import load_qa_chain
from langchain.llms import OpenAI
chain = load_qa_chain(OpenAI(), 
                      chain_type="stuff") # we are going to stuff all the docs in at once
```

### 运行链，构建查询

下一步，要构建的查询。首先，使用向量存储中返回的内容作为上下文片段来回答的问题。然后，将这个查询传给语言模型链。语言模型链会回答这个查询，给出相应的答案。

例如，可能会问 "这本书是哪些人创作的？"，然后将该查询传递给向量存储进行相似性搜索。系统会返回最相似的四个文档，将这些文档传递给语言模型链并给出查询，然后系统会给出一个答案。

```
query = "这本书是哪些人创作的？"
docs = docsearch.similarity_search(query)
chain.run(input_documents=docs, question=query)

```
看看他回答了什么:

```
' 不知道'
```


##### 选择返回的文档数量

可以设置返回的文档数量。默认情况下，系统会返回四个最相关的文档，但可以更改这个数字。

例如，可以设置返回前六个或更多的搜索结果。

```
query = "这本书的作者是谁"
docs = docsearch.similarity_search(query,k=6)
chain.run(input_documents=docs, question=query)
```

然而，需要注意的是，如果设置返回的文档数量过多, 比如设置 `k=20`，那么总的令牌数可能会超过模型的最大上下文长度，导致错误。例如，你使用的模型的最大上下文长度为 4097，但如果请求的令牌数超过了 5000，系统就会报错。

设置返回的文档数量为 `k=6`，获取的结果是：
```
'这本书的作者是Reid Hoffman和Sam Altman。'
```

### 7.3.6  进阶 map_rerank 链

为了解决这个问题，可以更改链类型。在之前的文章中看过许多不同的链类型。

"stuff" 类型优势是把所有内容都放在一起的地方。任何时候可以使用 "stuff"，最好就使用它，通用且节省成本。

还可以使用 "map_reduce" 在并行计算中对每个文档进行操作，但这可能会导致对 API 进行过多的调用，增加成本。

继续的讨论，将深入了解如何通过 Langchain 技术从 PDF 文档中提取有用的信息，特别是将重点讨论如何处理多个查询和理解返回结果。

第一种方式是使用不同类型的查询链类型。这里使用 `map_rerank` 这种类型，提高查询的质量。

###### `map_rerank` 优化查询质量

让从提出更复杂的查询开始。比如说，想要知道 "OpenAI 是什么"，并且想要获取前 10 个最相关的查询结果。在这种情况下，OpenAI 会返回多个答案，而不仅仅是一个。可以看到它不只返回一个答案，而是根据的需求返回了每个查询的答案和相应的评分。

```

from langchain.chains.question_answering import load_qa_chain

chain = load_qa_chain(OpenAI(), 
                      chain_type="map_rerank",
                      return_intermediate_steps=True
                      ) 

query = "OpenAI 的创始人是谁?"
docs = docsearch.similarity_search(query,k=10)
results = chain({"input_documents": docs, "question": query}, return_only_outputs=True)
results
```

重要的参数是 `return_intermediate_steps=True`, 设置这个参数可以看到 `map_rerank` 是如何对检索到的文档进行打分的。


######  理解评分系统

OpenAI 技术对返回的每个查询结果进行了评分。比如说，OpenAI 在这本书中被多次提及，因此它的评分可能会有 80 分，90 分甚至 100 分。可以假设 OpenAI 可能选择了评分为 100 分的两个或三个查询，然后将它们合并，最终给出了的输出。

```
{'intermediate_steps': [{'answer': ' This document does not answer the question.',
   'score': '0'},
  {'answer': ' OpenAI 的创始人是 Elon Musk, Sam Altman, Greg Brockman 和 Ilya Sutskever。 ',
   'score': '100'},
  {'answer': ' This document does not answer the question. ', 'score': '0'},
  {'answer': ' This document does not answer the question.', 'score': '0'},
  {'answer': ' This document does not answer the question.', 'score': '0'},
  {'answer': ' This document does not answer the question', 'score': '0'},
  {'answer': ' OpenAI 的创始人是 Elon Musk、 Sam Altman、 Greg Brockman、 Ilya Sutskever、Wojciech Zaremba 和 Peter Norvig。',
   'score': '100'},
  {'answer': ' This document does not answer the question.', 'score': '0'},
  {'answer': ' This document does not answer the question.', 'score': '0'},
  {'answer': ' This document does not answer the question', 'score': '0'}],
 'output_text': ' OpenAI 的创始人是 Elon Musk, Sam Altman, Greg Brockman 和 Ilya Sutskever。 '}
```

评分后，模型输出一个最终的答案, `'score': '100'` 得分 100 的那个答案：

```
results['output_text'] 
```

```
' OpenAI 的创始人是 Elon Musk, Sam Altman, Greg Brockman 和 Ilya Sutskever。 '
```

为了搞清楚为什么模型会评分，做出判断，可以打印 prompt 提示模板：

```
# check the prompt
chain.llm_chain.prompt.template
```
为了确保语言模型能够在接到问题后提供准确和有用的答案，为模型设计了一套详细的提示词。该提示词描述了如何根据给定的背景信息回答问题，并如何为答案打分。这部分强调了整体目标：使模型能够根据给定的背景信息提供准确答案，并为其答案打分。（行1-3）。

首先，模型需要明白其核心任务：根据给定的背景信息回答问题。如果模型不知道答案，它应直接表示不知道，而不是试图编造答案。这部分提醒模型，如果不知道答案，应该直接表示不知道，而不是编造答案。（行1）。

接下来，为模型提供了答案和评分的标准格式。答案部分要求模型简洁、明确地回答问题，而评分部分则要求模型为其答案给出一个0到100的分数，用以表示答案的完整性和准确性。这部分明确了答案和评分的格式，并强调了答案的完整性和准确性。（行7-10）。

此处，强调了答案的完整性和准确性是评分的核心标准，并为模型提供了三个示例来进一步说明如何评分。通过三个示例，模型可以更好地理解如何根据答案的相关性和准确性为其打分。（行17-48）。

最后，为了使模型能够在具体的实践中应用上述提示词，为模型提供了一个上下文背景和用户输入问题的模板。当模型接到一个问题时，它应使用此模板为问题提供答案和评分。（行50-57）。

下面是格式化和翻译过后的提示词模板。
```
1. 当你面对以下的背景信息时，如何回答最后的问题是关键。如果不知道答案，直接说你不知道，不要试图编造答案。
2. 
3. 除了提供答案外，还需要给出一个分数，表示它如何完全回答了用户的问题。请按照以下格式：
4. 
5. 问题：[这里的问题]
6. 
7. 有帮助的答案：[这里的答案]
8. 
9. 分数：[分数范围在0到100之间]
10. 
11. 如何确定分数：
12.    - 更高的分数代表更好的答案
13.    - 更好的答案能够充分地回应所提出的问题，并提供足够的细节
14.    - 如果根据上下文不知道答案，那么分数应该是0
15.    - 不要过于自信！
16. 
17. 示例 #1
18. 
19. 背景：
20.    - 苹果是红色的
21. 
22. 问题：苹果是什么颜色？
23. 
24. 有帮助的答案：红色
25. 
26. 分数：100
27. 
28. 示例 #2
29. 
30. 背景：
31.    - 那是夜晚，证人忘了带他的眼镜。他不确定那是一辆跑车还是SUV
32. 
33. 问题：那辆车是什么类型的？
34. 
35. 有帮助的答案：跑车或SUV
36. 
37. 分数：60
38. 
39. 示例 #3
40. 
41. 背景：
42.    - 梨要么是红色的，要么是橙色的
43. 
44. 问题：苹果是什么颜色？
45. 
46. 有帮助的答案：这个文档没有回答这个问题
47. 
48. 分数：0
49. 
50. 开始！
51. 
52. 背景：
53.    - {context}
54. 
55. 问题：{question}
56. 
57. 有帮助的答案：
```



### 7.3.7  RetrievalQA 链

除了单个查询，还可以使用链式查询。可以开始将这些查询放在一个链条中。

链式查询指的是查询向量存储链和语言模型的链。

例如可以有一个查询向量存储的链，以及一个查询语言模型的链。然后，可以将这些链条组合在一起，创建一个RetrievalQA 链 。


##### 使用 RetrievalQA 链

RetrievalQA 链是 Langchain 已经封装好的索引查询问答链。实例化之后，可以直接把问题扔给它，而不需要 `chain.run()`。简化了很多步骤，获得了比较稳定的查询结果。

为了创建这样的链，需要一个检索器。可以使用之前设置好的 docsearch, 作为检索器，并且可以设置返回的文档数量 `"k":4`。

```
docsearch = FAISS.from_texts(texts, embeddings) # 检索器是向量库数据
```

可以将这些参数传递给链条类型 "stuff"，它会为返回源文档。（选择 "stuff" 类型的原因：跟第一个 "stuff" 类型 和 `map_reduce` 类型对比答案的质量）。


```
from langchain.chains import RetrievalQA

# set up FAISS as a generic retriever 
retriever = docsearch.as_retriever(search_type="similarity", search_kwargs={"k":4})  # as_retriever方法是构建检索器 

# create the chain to answer questions 
rqa = RetrievalQA.from_chain_type(llm=OpenAI(), 
                                  chain_type="stuff", 
                                  retriever=retriever, 
                                  return_source_documents=True)
```

##### 返回结果和源文档

当查询 "OpenAI 是什么" 时，不仅会得到一个答案，还会得到源文档 `source_documents`。源文档是返回结果的参考文档，它可以帮助理解答案是如何得出的。

```
query = "OpenAI 是什么?"
rqa(query)['result']
```

```
'OpenAI 是一家技术研究和开发公司，旨在研究人工智能的安全性、可可控性和效率。它的主要目标是使智能技术得以广泛使用，以改善人类生活。' 
```

##### 直接返回结果设置

如果不需要中间步骤和源文档，只需要最终答案，那么可以直接请求返回结果。将代码:

```
return_source_documents=True
```
改为
```
return_source_documents=False
```

比如说，问 "gpt-4 对创新力有什么影响?"

```
query = "gpt-4 对创新力有什么影响?"
rqa(query)['result']
```
它会直接返回结果，不包括源文档。

```
' GPT-4可以加强创作者和创作者的创作能力和生产力，从而提高创新力。它可以帮助他们与任务，例如头脑风暴，编辑，反馈，翻译和营销。此外，GPT-4还可以帮助他们更快地完成任务，从而提高他们的生产效率。它也可以帮助他们更深入地思考，更有创意地思考，
```










