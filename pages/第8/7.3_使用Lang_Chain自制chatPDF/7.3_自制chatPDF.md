
# 用 LangChain 自制 chatPDF

我们这个 ChatPDF 项目是引入外部数据集对大语言模型进行微调，以生成更准确的回答的程序。

试想你是一个航天飞机设计师，你需要了解最新的航空材料技术，你可以将这个需求输入到我们的模型中，模型就会根据最新的数据集给出准确的答案。

我们的界面呈现的是人类与文档问答的聊天，但实质上，我们仍然是在与大语言模型交流，只不过这个模型现在被赋予了接入外部数据集的能力。就像你在与一位熟悉你公司内部文档的同事交谈，尽管他可能并未参与过这些文档的编写，但他可以准确地回答你的问题。

大语言模型之前，我们不能像聊天一样与文档交流，我们只能依赖于搜索。例如, 你正在为一项重要的报告寻找资料，你必须知道你需要查找的关键词，然后在大量的信息中筛选出你需要的部分。而现在，我们可以通过聊天的方式，即使不知道具体的关键词，也可以让模型根据我们的问题告诉我们答案。就好像你在问一位专业的图书馆员，哪些书籍可以帮助你完成这份报告。

那为什么我们要引入文档的外部数据集呢？这是因为大语言模型的训练数据截止到 2021 年 9 月，之后产生的知识和信息并未被包含进去。就像我们的模型是一个生活在过去的时间旅行者，他只能告诉你他离开的那个时刻之前的所有信息，对之后的事情一无所知。

我们的模型训练数据不仅包含参数，也引入了外部数据集进行训练。就好像我们通过电话，向一个在远方的朋友讲述最新的新闻，这样他就可以了解到最新的事情。我们的大语言模型也一样，通过外部数据集的输入，我们可以使它了解到最新的信息，从而进行更好的检索增强和微调，适应我们的实时需求。

引入外部数据集还有一个重要的目的，那就是修复大语言模型的 "机器幻觉"，避免给出错误的回答。试想一下，如果你向一个只知道过去信息的人询问未来的趋势，他可能会基于过去的信息进行推断，但这样的答案未必正确。所以我们通过引入最新的数据，让我们的模型能够更准确地回答问题，避免因为信息过时产生的误导。

我们现在使用的数据文档形式包括 pdf、json、word、excel 等，这些都是我们获取实时知识和数据的途径。这类程序现在非常受欢迎，比如最著名的 chatpdf 和 chatdoc, 还有针对各种特定领域的程序，如针对法律文档的程序。就像你在阅读各种格式的书籍一样，不同的程序能够提供不同的知识和信息。



### 7.3.1 程序流程

我们的实现方式是利用 Langchain 已实现的向量存储、嵌入以及使用查询和检索相关的链，来获取外部数据集，处理文档，进行相关性检索后合并处理，置入大语言模型的提示模板中，实现与 PDF 文件交流的目的。

我们选定的文档是 Reid Hoffman 写的一本关于 GPT-4 和人工智能的书，我们将下载这本 PDF 并将其转化为可查询和交互的形式。

#### 加载文件且切分块

首先，我们要加载 PDF 文件，并将其分割成块。接下来，我们将创建一个向量存储，并对其进行查询。这个过程很像使用搜索引擎，我们在向量存储中查找相关内容，但与基于关键词的搜索不同，我们是基于语义的。

#### 嵌入和向量存储

为每个块创建嵌入，这个嵌入会是一个向量，代表该块中的所有信息。当我们需要查询某个信息时，我们会将问题嵌入为一个向量，然后发送给向量存储器。向量存储器会返回最相关的信息块，然后将其与问题一起传递给语言模型。

#### 语言模型 I/O

语言模型会阅读这些信息，并根据这些信息决定答案是什么。这种基于语义搜索的概念并不新，甚至使用大语言模型来查询文本的想法也已经存在很长时间了。

#### LangChain 创建链

我们需要做的关键步骤包括：加载文档，分割文档，创建嵌入，将嵌入放入向量存储。接下来，我们将使用 LangChain 创建我们的链，以便我们可以进行查询，查找向量存储中的信息，将其带入链中，然后与问题结合，最终得出一个答案。



### 7.3.2 环境和工具

#### 安装包
```
!pip -q install langchain openai tiktoken PyPDF2 faiss-cpu

```
#### 设置密钥
```

import os

os.environ["OPENAI_API_KEY"] = ""
```

#### 下载 PDF 文本 
```
# 可以复制到浏览器后，直接保存在本地电脑上。
!wget -q https://www.impromptubook.com/wp-content/uploads/2023/03/impromptu-rh.pdf
```

### 导入库和 PDF 加载器

首先，我们需要一个 PDF 阅读器。虽然我们这次只使用了一个基础的 PDF 阅读器，你也可以根据需要选择更合适的 PDF 阅读器。我们通过 PDF 阅读器将 PDF 文档读取成一个长字符串。这个过程可能会遇到一些格式问题，比如奇怪的空格等，但每个项目都有自己独特的数据处理方法。不论是使用基础的处理方式，还是使用诸如'unstructured'库、AWS 或 Google Cloud 的 API，都有可能。


先导入阅读器 `PdfReader`, 嵌入模型 `OpenAIEmbeddings`, 文档切分器 `CharacterTextSplitter`, 向量存储库 `FAISS`  ： 

```
from PyPDF2 import PdfReader
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores import FAISS 
```

加载之前准备好的 PDF 素材：

```
# 括号内填入当前电脑保存pdf文件的位置. 
doc_reader = PdfReader('/content/impromptu-rh.pdf')
```
打印 `doc_reader` 看看 :
```
<PyPDF2._reader.PdfReader at 0x7f119f57f640>
```
将 PDF 文档转化为可用的 `raw_text` 格式：

```
# read data from the file and put them into a variable called raw_text
raw_text = ''
for i, page in enumerate(doc_reader.pages):
    text = page.extract_text()
    if text:
        raw_text += text
```     
我们可以打印 `raw_text` 的结果字符串长度，看看是不是转换成功了：

```
len(raw_text) # 得到 356710 的结果
```


### 7.3.3 文本拆分

我们需要将获取到的长字符串拆分为适合分析的小段落。

我们的方法很简单，就是将这个长字符串按照字符数拆分。比如我们可以设定每 1000 个字符为一个块, `chunk_size = 1000`。


```

# Splitting up the text into smaller chunks for indexing
text_splitter = CharacterTextSplitter(        
    separator = "\n",
    chunk_size = 1000,
    chunk_overlap  = 200, #striding over the text
    length_function = len,
)
texts = text_splitter.split_text(raw_text)

```

我们总共切了 448 个块：

```
len(texts) # 448
``` 

*注意*： 在这个代码片段中，`chunk_overlap` 参数用于指定文本切分时的重叠量（overlap）。它表示在切分后生成的每个分块之间重叠的字符数。具体来说，这个参数表示每个分块的前后，两个分块之间会有多少个字符是重复的。举例来说 chunkA 和 chunkB, 他们有 200 个字符是重复的。

然后，我们采用滑动窗口的方法来拆分文本。即每个块之间会有部分字符重叠，比如在每 1000 个字符的块上，我们让前后两块有 200 个字符重叠。这样做的目的是避免关键信息被切分，而且即使有些信息出现在了多个块中，因为我们是在获取整体语义，所以这些重叠的块在语义上也会有所区别。

我们可以随机打印一块的内容：

```
texts[20]
```

输出是：

```
'million registered users. \nIn late January 2023, Microsoft1—which had invested $1 billion \nin OpenAI in 2019—announced that it would be investing $10 \nbillion more in the company. It soon unveiled a new version of \nits search engine Bing, with a variation of ChatGPT built into it.\n1 I sit on Microsoft’s Board of Directors. 10Impromptu: Amplifying Our Humanity Through AI\nBy the start of February 2023, OpenAI said ChatGPT had \none hundred million monthly active users, making it the fast-\nest-growing consumer internet app ever. Along with that \ntorrent of user interest, there were news stories of the new Bing \nchatbot functioning in sporadically unusual ways that were \nvery different from how ChatGPT had generally been engaging \nwith users—including showing “anger,” hurling insults, boast-\ning on its hacking abilities and capacity for revenge, and basi-\ncally acting as if it were auditioning for a future episode of Real \nHousewives: Black Mirror Edition .'
```



### 7.3.4  创建嵌入和检索

有了分好的小块文本，我们就可以为这些文本创建嵌入了。在这个步骤，我们使用了 OpenAI 的嵌入技术。

```
# Download embeddings from OpenAI
embeddings = OpenAIEmbeddings()
```
我们先把文本传给嵌入制造器，然后通过 FAISS 库创建向量存储本身。

```
docsearch = FAISS.from_texts(texts, embeddings)
```

至此，我们已经将原本的 PDF 文档转化为了可以进行机器学习的向量数据。

就是这么简单，接下来我们就可以向这个 PDF 问问题了。

### 相似度检索

现在，我们可以使用这些向量数据来进行搜索匹配了。我们以一个实际的查询为例：“GPT-4 如何改变社交媒体？”。

```
query = "how does GPT-4 change social media?"
docs = docsearch.similarity_search(query) # 这是搜索匹配的文字结果数组。
```

我们把这个查询传给文档搜索系统，使用相似度匹配搜索 `similarity_search`，在向量数据中寻找与查询最匹配的文档。

我们的搜索结果将包括与查询最接近的四个文档，而这些文档都是通过我们的嵌入函数进行嵌入的。

```

len(docs)  # 结果为 4 代表有 4 处地方跟问题有关系

```

我们尝试打印第一个 `docs[0]`
在我们的搜索结果中，首位的文档中多次提到了“社交媒体”，看来我们的查询效果还是很好的。

```
Document(page_content='rected ways that tools like GPT-4 and DALL-E 2 enable.\nThis is a theme I’ve touched on throughout this travelog, but \nit’s especially relevant in this chapter. From its inception, social \nmedia worked to recast broadcast media’s monolithic and \npassive audiences as interactive, democratic communities, in \nwhich newly empowered participants could connect directly \nwith each other. They could project their own voices broadly, \nwith no editorial “gatekeeping” beyond a given platform’s terms \nof service.\nEven with the rise of recommendation algorithms, social media \nremains a medium where users have more chance to deter -\nmine their own pathways and experiences than they do in the \nworld of traditional media. It’s a medium where they’ve come \nto expect a certain level of autonomy, and typically they look for \nnew ways to expand it.\nSocial media content creators also wear a lot of hats, especially \nwhen starting out. A new YouTube creator is probably not only', metadata={})
```

这就是如何利用 OpenAI 技术处理 PDF 文档，将海量的信息提炼为可用的数据的全部步骤。是不是很简单，赶紧动手做起来吧~

我们现在只有一个 PDF 文档，实现代码也很简单，Langchain 给了很多组件，我们完成得很快。接下来，我们处理多文档的提问，现实是我们要获取到真实的信息，通过会跨越多个文档，才能提取有用的信息。比如读取金融研报，新闻综合报道等等。


### 7.3.5  进阶 Stuff 链

在上一节中，我们加载了一个 PDF 文档，转化格式，切分字符后，创建向量数据来进行搜索匹配获得了问题的答案。一旦我们有了已经处理好的文档，我们就可以开始构建一个简单的问答链。现在我们看看 如何使用 Langchain 构建问答链。 

在这个过程中，我们使用了 OpenAI 的模型，并选择了 Langchain 的现有文档处理链中 一种被称为 "stuff" 的链类型。在这种模式下，我们只是将所有内容都放在一个调用中，理想情况下，我们放入的内容应该少于 4000 个令牌。

除了 "stuff" 之外，Langchain 文档处理链还有 精化（Refine）、Map reduce 、重排（Map re-rank）。后面我们会再次用到。

```

from langchain.chains.question_answering import load_qa_chain
from langchain.llms import OpenAI
chain = load_qa_chain(OpenAI(), 
                      chain_type="stuff") # we are going to stuff all the docs in at once
```

### 运行链，构建查询

下一步，我们要构建我们的查询。首先，我们使用向量存储中返回的内容作为上下文片段来回答我们的问题。然后，我们将这个查询传给语言模型链。语言模型链会回答这个查询，给出相应的答案。

例如，我们可能会问 "这本书的作者是谁？"，然后将该查询传递给向量存储进行相似性搜索。系统会返回最相似的四个文档，我们将这些文档传递给语言模型链并给出查询，然后系统会给出一个答案。

```
query = "who are the authors of the book?"
docs = docsearch.similarity_search(query)
chain.run(input_documents=docs, question=query)

```
看看他回答了什么:

```
' The authors of the book are Reid Hoffman and Ben Casnocha.'
```


#### 选择返回的文档数量

我们可以设置返回的文档数量。默认情况下，系统会返回四个最相关的文档，但我们可以更改这个数字。

例如，我们可以设置返回前六个或更多的搜索结果。

```
query = "who is the book authored by?"
docs = docsearch.similarity_search(query,k=6)
chain.run(input_documents=docs, question=query)
```

然而，我们需要注意的是，如果我们设置返回的文档数量过多, 比如设置 `k=20`，那么总的令牌数可能会超过模型的最大上下文长度，导致错误。例如，你使用的模型的最大上下文长度为 4097，但如果我们请求的令牌数超过了 5000，系统就会报错。

设置返回的文档数量为 `k=6`，我们获取的结果是：
```
' The book is authored by Reid Hoffman and Aria Finger.'
```

### 7.3.6  进阶 map_rerank 链

为了解决这个问题，我们可以更改链类型。我们在之前的文章中看过许多不同的链类型。

"stuff" 类型优势是把所有内容都放在一起的地方。任何时候我们可以使用 "stuff"，最好就使用它，通用且节省成本。

我们还可以使用 "map_reduce" 在并行计算中对每个文档进行操作，但这可能会导致对 API 进行过多的调用，增加成本。

继续我们的讨论，我们将深入了解如何通过 Langchain 技术从 PDF 文档中提取有用的信息，特别是我们将重点讨论如何处理多个查询和理解返回结果。

第一种方式是使用不同类型的查询链类型。这里我们使用 `map_rerank` 这种类型，提高查询的质量。

##### `map_rerank` 优化查询质量

让我们从提出更复杂的查询开始。比如说，我们想要知道 "OpenAI 是什么"，并且我们想要获取前 10 个最相关的查询结果。在这种情况下，OpenAI 会返回多个答案，而不仅仅是一个。我们可以看到它不只返回一个答案，而是根据我们的需求返回了每个查询的答案和相应的评分。

```

from langchain.chains.question_answering import load_qa_chain

chain = load_qa_chain(OpenAI(), 
                      chain_type="map_rerank",
                      return_intermediate_steps=True
                      ) 

query = "who are openai?"
docs = docsearch.similarity_search(query,k=10)
results = chain({"input_documents": docs, "question": query}, return_only_outputs=True)
results
```

重要的参数是 `return_intermediate_steps=True`, 设置这个参数我们可以看到 `map_rerank` 是如何对检索到的文档进行打分的。


#####  理解评分系统

OpenAI 技术对返回的每个查询结果进行了评分。比如说，OpenAI 在这本书中被多次提及，因此它的评分可能会有 80 分，90 分甚至 100 分。我们可以假设 OpenAI 可能选择了评分为 100 分的两个或三个查询，然后将它们合并，最终给出了我们的输出。

```
{'intermediate_steps': [{'answer': ' OpenAI is an organization that released text-to-image generation tool DALL-E 2 and ChatGPT in April 2022 and are giving millions of users hands-on access to these AI tools.',
   'score': '80'},
  {'answer': ' OpenAI is a research laboratory whose founding goal is to develop technologies that put the power of AI directly into the hands of millions of people.',
   'score': '80'},
  {'answer': ' OpenAI is a research organization that develops and shares artificial intelligence tools for the benefit of humanity.',
   'score': '100'},
  {'answer': ' OpenAI is a technology company focused on using artificial intelligence to solve real-world problems.',
   'score': '80'},
  {'answer': ' OpenAI is a company co-founded by Sam Altman that is developing AI technologies and allowing individuals to participate in the development process.',
   'score': '90'},
  {'answer': ' OpenAI is a research laboratory that focuses on artificial intelligence technologies.',
   'score': '80'},
  {'answer': ' OpenAI is a non-profit artificial intelligence research company that was founded in 2015 with the goal of advancing digital intelligence in the way that is most likely to benefit humanity as a whole. ',
   'score': '90'},
  {'answer': ' OpenAI is a nonprofit artificial intelligence (AI) research organization. Microsoft invested $1 billion in OpenAI in 2019 and announced in late January 2023 that it would be investing an additional $10 billion.',
   'score': '100'},
  {'answer': ' OpenAI is an artificial intelligence research laboratory founded by Elon Musk and Sam Altman.',
   'score': '100'},
  {'answer': ' OpenAI is an artificial intelligence research laboratory founded in December 201It is a non-profit organization with the mission to ensure that artificial general intelligence (AGI) benefits all of humanity. ',
   'score': '90'}],
 'output_text': ' OpenAI is a research organization that develops and shares artificial intelligence tools for the benefit of humanity.'}
```

评分后，模型输出一个最终的答案, `'score': '100'` 得分 100 的那个答案：

```
results['output_text'] 
```

```
' OpenAI is a research organization that develops and shares artificial intelligence tools for the benefit of humanity.'
```

为了搞清楚为什么模型会评分，做出判断，我们可以打印 prompt 提示模板：

```
# check the prompt
chain.llm_chain.prompt.template
```
看完模板的一刻，你肯定有所顿悟：

```
"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n\nIn addition to giving an answer, also return a score of how fully it answered the user's question. This should be in the following format:\n\nQuestion: [question here]\nHelpful Answer: [answer here]\nScore: [score between 0 and 100]\n\nHow to determine the score:\n- Higher is a better answer\n- Better responds fully to the asked question, with sufficient level of detail\n- If you do not know the answer based on the context, that should be a score of 0\n- Don't be overconfident!\n\nExample #1\n\nContext:\n---------\nApples are red\n---------\nQuestion: what color are apples?\nHelpful Answer: red\nScore: 100\n\nExample #2\n\nContext:\n---------\nit was night and the witness forgot his glasses. he was not sure if it was a sports car or an suv\n---------\nQuestion: what type was the car?\nHelpful Answer: a sports car or an suv\nScore: 60\n\nExample #3\n\nContext:\n---------\nPears are either red or orange\n---------\nQuestion: what color are apples?\nHelpful Answer: This document does not answer the question\nScore: 0\n\nBegin!\n\nContext:\n---------\n{context}\n---------\nQuestion: {question}\nHelpful Answer:"
```


### 7.3.7  RetrievalQA 链

除了单个查询，我们还可以使用链式查询。我们可以开始将这些查询放在一个链条中。

链式查询指的是查询向量存储链和语言模型的链。

例如我们可以有一个查询向量存储的链，以及一个查询语言模型的链。然后，我们可以将这些链条组合在一起，创建一个检索 QA 链条 。


#### 使用 RetrievalQA 链

RetrievalQA 链是 Langchain 已经封装好的索引查询问答链。实例化之后，我们可以直接把问题扔给它，而不需要 `chain.run()`。简化了很多步骤，获得了比较稳定的查询结果。

为了创建这样的链，我们需要一个检索器。我们可以使用之前设置好的 docsearch, 作为检索器，并且我们可以设置返回的文档数量 `"k":4`。

```
docsearch = FAISS.from_texts(texts, embeddings) # 检索器是向量库数据
```

我们可以将这些参数传递给链条类型 "stuff"，它会为我们返回源文档。（选择 "stuff" 类型的原因：跟第一个 "stuff" 类型 和 `map_reduce` 类型对比答案的质量）。


```
from langchain.chains import RetrievalQA

# set up FAISS as a generic retriever 
retriever = docsearch.as_retriever(search_type="similarity", search_kwargs={"k":4})  # as_retriever方法是构建检索器 

# create the chain to answer questions 
rqa = RetrievalQA.from_chain_type(llm=OpenAI(), 
                                  chain_type="stuff", 
                                  retriever=retriever, 
                                  return_source_documents=True)
```

#### 返回结果和源文档

当我们查询 "OpenAI 是什么" 时，我们不仅会得到一个答案，还会得到源文档 `source_documents`。源文档是返回结果的参考文档，它可以帮助我们理解答案是如何得出的。

```
rqa("What is OpenAI?")
```

```
{'query': 'What is OpenAI?',
 'result': ' OpenAI is a research organization that develops and shares artificial intelligence tools for the benefit of humanity.',
 'source_documents': [Document(page_content='thing that has largely been happening to individuals rather \nthan for them—an under-the-radar force deployed by Big Tech \nwithout much public knowledge, much less consent, via tech -\nnologies like facial recognition and algorithmic decision-mak-\ning on home loans, job applicant screening, social media rec-\nommendations, and more.\nA founding goal of OpenAI was to develop technologies that put \nthe power of AI directly into the hands of millions of people..... #😊后面我省略了。
```

#### 直接返回结果设置

如果我们不需要中间步骤和源文档，只需要最终答案，那么我们可以直接请求返回结果。将代码:

```
return_source_documents=True
```
改为
```
return_source_documents=False
```

比如说，我们问 "What does gpt-4 mean for creativity?"

```
query = "What does gpt-4 mean for creativity?"
rqa(query)['result']
```
它会直接返回结果，不包括源文档。

```
' GPT-4 can amplify the creativity of humans by providing contextualized search, versatile brainstorming and production aid, and the ability to generate dialogue and branching narratives for interactive characters.'
```





