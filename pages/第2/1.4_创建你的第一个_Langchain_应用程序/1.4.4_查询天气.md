# 1.4.4 开始对话

当我们的生活越来越依赖于各种信息，比如我们可能想要去郊游，需要查询当天的天气状况，路况信息等，这时候，我们的聊天机器人就可以发挥巨大的作用。不仅如此，他甚至可以帮我们制定计划。那么，如何让聊天机器人完成这样的任务呢？这就需要借助 Langchain 的结构化组件：“代理”。 

#### 代理

“代理”在 LangChain 中，是目前最先进的模块，它的主要职责是基于输入的信息，动态地选择执行哪些动作，以及确定这些动作的执行顺序。一个代理会被赋予一些“工具”，这些工具可以执行特定的任务。代理会反复选择一个工具，运行这个工具，观察输出结果，直到得出最终的答案。换句话说，代理就像一个决策者，它决定使用什么工具来获取天气信息，而我们只需要关注它给我们的最终答案。

要创建并加载一个代理，你需要选择以下几个要素：

1. 大语言模型（LLM）或聊天模型：这是驱动代理的语言模型。
2. 工具（Tool）：执行特定任务的函数。例如，谷歌搜索、数据库查询、Python REPL，甚至其它模型链。对于预定义的工具及其规格，可以查看工具文档。
3. 代理名称：引用受支持的代理类的字符串。代理类主要由语言模型用于决定执行哪个动作的提示模板参数化。

在以下的代码示例中，我们将使用 SerpAPI 查询搜索引擎来创建一个代理：

安装库。
```
pip -q install  openai
pip install git+https://github.com/hwchase17/langchain
```
设置密钥。

```
# 设置OpenAI的API密钥
os.environ[“OPENAI_API_KEY”] = “”
# 设置谷歌搜索的API密钥
os.environ[“SERPAPI_API_KEY”] = “”
```

```python
from langchain.agents import load_tools
from langchain.agents import initialize_agent
from langchain.agents import AgentType
from langchain.chat_models import ChatOpenAI
from langchain.llms import OpenAI

# 首先，加载控制代理的语言模型
chat = ChatOpenAI(temperature=0)""

# 加载一些工具，注意这里的`llm-math`工具使用了一个LLM，因此需要将其传入
llm = OpenAI(temperature=0)
tools = load_tools([“serpapi”, “llm-math”], llm=llm)

# 最后，用工具、语言模型以及我们想要使用的代理类型初始化一个代理
agent = initialize_agent(tools, chat, agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION, verbose=True)

# 现在我们测试一下代理
agent.run(“What will be the weather in Shanghai three days from now?”)
```

通过以上步骤，我们成功创建并运行了一个代理，这个代理能够帮助我们从网络上获取信息，并进行一些数学计算。这样，无论我们想要查询天气、路况，还是计划郊游，我们都可以轻松地通过这个聊天机器人得到所需的信息。

#### 记忆

在此之前，我们制造的机器人虽然已经能使用工具进行搜索，进行数学运算，但它仍然是无状态的。这意味着它无法引用过去的交互，也就无法根据过去的交互理解新的消息。这显然对于聊天机器人来说是不足的，因为我们希望机器人能够理解新消息，并在此基础上理解过去的消息。

为了解决这个问题，langchain 提供了一个记忆模块。记忆模块提供了一种维持应用状态的方式。这个基础的记忆界面非常简单：它允许我们根据最新的运行输入和输出更新状态，并允许我们利用存储的状态修改或上下文化下一个输入。

在内置的记忆系统中，最简单的就是缓冲记忆。缓冲记忆只是将最近的一些输入/输出预置到当前的输入中。我们可以用代码来看这个过程：

首先，我们需要从 langchain。prompts 导入一些类和函数。然后，我们创建一个 ChatOpenAI 对象，这是我们的语言模型。

```python
from langchain.prompts import (
    ChatPromptTemplate,
    MessagesPlaceholder,
    SystemMessagePromptTemplate,
    HumanMessagePromptTemplate
)
from langchain.chains import ConversationChain
from langchain.chat_models import ChatOpenAI
from langchain.memory import ConversationBufferMemory

prompt = ChatPromptTemplate.from_messages([
    SystemMessagePromptTemplate.from_template(
        “The following is a friendly conversation between a human and an AI. The AI is talkative and ”
        “provides lots of specific details from its context. If the AI does not know the answer to a ”
        “question, it truthfully says it does not know.”
    ),
    MessagesPlaceholder(variable_name=“history”),
    HumanMessagePromptTemplate.from_template(“{input}”)
])

llm = ChatOpenAI(temperature=0)
```


接着，我们创建一个 ConversationBufferMemory 对象，这是我们的记忆。

```
memory = ConversationBufferMemory(return_messages=True)
```

最后，我们创建一个 ConversationChain 对象，它是我们的会话链，会话链会用到之前创建的记忆和语言模型。

```python
conversation = ConversationChain(memory=memory, prompt=prompt, llm=llm)
```

创建了会话链之后，我们就可以用它来预测输入了。

```python
conversation.predict(input=“你好，我是美丽!”)
```

例如，我们可以向会话链输入 “你好，我是美丽!”，然后会话链就会根据存储的状态和输入，生成一个响应。由于我们的记忆模型是缓冲记忆，所以会话链的响应会考虑到最近的一些输入/输出。

在后面的对话中，机器人会记住我们的名字。相反我们可以给机器人取一个特别的名字，因为有记忆的存在，机器人会记住他的名字。

总的来说，通过使用记忆模块，我们的聊天机器人不仅可以进行搜索和数学运算，还能引用过去的交互，理解新的消息。这大大提高了聊天机器人的实用性和智能水平。

祝贺大家，我们的第一个聊天机器人现已完成。