# 1.4.3 开始翻译

首先，我们需要安装他们的 Python 包：

```shell
pip install openai langchain
```

访问 API 需要一个 API 密钥，你可以通过创建一个账户并访问此处获得。一旦我们得到密钥，我们会想要将其设置为环境变量，通过运行：

```shell
export openai_api_key=""
```
#### 聊天模型

LangChain 的 schema 定义了 AIMessage、HumanMessage 和 SystemMessage 这三种角色类型的数据模式。这些都是我们设计的数据模型，通过这些模型，我们可以像使用函数一样将参数传递给它们。

例如，如果我们想要与聊天机器人进行对话，我们只需要把想要说的话用 HumanMessage 函数封装起来，像这样：`HumanMessage(content="你好!")`。然后我们将这个消息放入一个列表中，传递给 ChatModel 模型。这样，我们就可以开始与聊天机器人进行交流了。

如果我们想要使用这个聊天机器人来翻译一段英文，我们可以这样编写代码：

```python
from langchain.chat_models import ChatOpenAI
from langchain.schema import (
    AIMessage,
    HumanMessage,
    SystemMessage
)

chat = ChatOpenAI(temperature=0)
chat.predict_messages([HumanMessage(content=“Translate this sentence from English to French. I love programming.”)])
# 输出：AIMessage(content=“J'aime programmer.”, additional_kwargs={})
```

在这段代码中，我们首先导入了需要的模块和函数。然后，我们创建了一个 ChatOpenAI 对象，并且设置了温度参数为 0，这意味着模型的输出将会具有更低的随机性。之后，我们调用 `chat.predict_messages` 方法，向它传递了一个包含 HumanMessage 对象的列表。这个 HumanMessage 对象包含了我们想要翻译的英文句子。最后，我们的模型将返回一个 AIMessage 对象，它包含了这句英文的法文翻译。
```

#### 提示模板

提示模板是一种特殊的文本，它可以为特定任务提供额外的上下文信息。在大语言模型（LLM）的应用中，通常并不直接将用户的输入传递给LLM，而是将用户输入添加到一个更大的文本中，即提示模板。提示模板为当前的具体任务提供了额外的上下文信息，这能够更好地引导模型生成预期的输出。

如何使用提示模板？

在LangChain中，我们可以使用MessagePromptTemplate来创建提示模板。我们可以从一个或多个MessagePromptTemplates创建一个ChatPromptTemplate。示例代码如下：

```
from langchain.prompts.chat import (
    ChatPromptTemplate,
    SystemMessagePromptTemplate,
    HumanMessagePromptTemplate,
)

template = “You are a helpful assistant that translates {input_language} to {output_language}.”
system_message_prompt = SystemMessagePromptTemplate.from_template(template)
human_template = “{text}”
human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)

chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])

chat_prompt.format_messages(input_language = “English”, output_language = “French”, text = “I love programming.”)

```

在上述代码中，我们首先定义了两种模板：一个是系统消息模板，描述了任务的上下文（翻译助手的角色和翻译任务）；另一个是人类消息模板，这将会是用户的输入。

然后，我们使用ChatPromptTemplate。from_messages方法，将这两个模板结合起来，生成了一个聊天提示模板。

当我们需要生成预期的输出时，我们可以调用ChatPromptTemplate的format_messages方法，像这样：

```
[
    SystemMessage(content = “You are a helpful assistant that translates English to French.”, additional_kwargs ={}),
    HumanMessage(content = “I love programming.”)
]
```
通过这种方式，我们不仅可以生成预期的输出，还能让用户无需担心提供模型指令，他们只需要提供具体的任务信息即可。

#### 创建第一个链

现在，让我们将上述步骤整合为一条链，以此创建我们的第一个链。我们将使用LangChain的LLMChain（大语言模型链）对模型进行包装，实现与提示模板类似的功能。这种方式更为直观易懂，你会发现我们导入了一个包装链 LLMChain， 将提示模板和模型传递进去后，我们就造好了链。而链的运行只要 run 一下。以下是相关代码：

``` python
from langchain import LLMChain
from langchain.chat_models import ChatOpenAI
from langchain.prompts.chat import (
    ChatPromptTemplate,
    SystemMessagePromptTemplate,
    HumanMessagePromptTemplate,
)

# 初始化 ChatOpenAI 聊天模型，温度设置为 0
chat = ChatOpenAI(temperature = 0)

# 定义系统消息的模板
template = “You are a helpful assistant that translates {input_language} to {output_language}.”
system_message_prompt = SystemMessagePromptTemplate.from_template(template)

# 定义人类消息的模板
human_template = “{text}”
human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)

# 将这两种模板组合到聊天提示模板中
chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])

# 使用 LLMChain 包装模型和提示模板
chain = LLMChain(llm = chat, prompt = chat_prompt)

# 运行模型链，传入参数
chain.run(input_language = “English”, output_language = “French”, text = “I love programming.”)
```

在这段代码中，我们首先初始化了一个ChatOpenAI聊天模型，然后定义了系统消息模板和人类消息模板，并将它们组合在一起创建了一个聊天提示模板。接着，我们使用LLMChain来包装我们的聊天模型和聊天提示模板。最后，我们运行了这个模型链，并传入了参数。这样，我们就可以方便地运行模型，并且不需要每次都为提示模板提供所有的参数。