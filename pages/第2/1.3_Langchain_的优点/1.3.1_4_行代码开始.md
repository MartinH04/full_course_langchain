# 1.3.1 四行代码开始


使用 LangChain 通常需要集成一个或多个模型提供商，数据存储，API 等。在这个例子中，我们将使用 OpenAI 的模型 API。

首先，我们需要安装他们的 Python 包：

```shell
pip install openai langchain
```

访问 API 需要一个 API 密钥，你可以通过创建一个账户并访问此处获得。一旦我们得到密钥，我们会想要将其设置为环境变量，通过运行：

```shell
export OPENAI_API_KEY=“...”
```

如果你不会设置环境变量，你可以在初始化 OpenAI LLM 类时，直接通过 `openai_api_key` 参数将密钥传入：

```python
from langchain.llms import OpenAI

llm = OpenAI(openai_api_key=“...”)
```

### 构建应用程序

现在我们可以开始构建我们的语言模型应用程序。LangChain 提供了许多可以用来构建语言模型应用程序的模块。模块可以在简单应用程序中单独使用，也可以在更复杂的使用场景中组合使用。

#### LLMs

从语言模型获取预测结果

LangChain 的基本构建块是 LLM，它接收文本并生成更多的文本。

例如，假设我们正在构建一个根据公司描述生成公司名称的应用程序。为了做到这一点，我们需要初始化一个 OpenAI 模型包装器。在这种情况下，因为我们希望输出更随机和创意性，所以我们会用 “temperature” 来初始化我们的模型。

值得注意的是，模型的 `temperature` 参数。`temperature` 的值范围在 0-1 之间。此参数用于控制生成的文本的随机性。当 `temperature` 设置为 0 时，模型产生的结果会具有更低的随机性，通常更确定性和连贯性。相反，当 `temperature` 设置为 1 时，模型产生的结果会更加随机，这可能会产生更有创新性和多样性的结果。

```python
from langchain.llms import OpenAI

llm = OpenAI(temperature=0.9)
```

现在我们可以输入问题，并且获得答案了！

```python
llm.predict(“What would be a good company name for a company that makes colorful socks?”)
# >> Feetful of Fun
```

#### 聊天模型

聊天模型是语言模型的一种变体。虽然聊天模型在底层使用语言模型，但是它们暴露的接口有所不同：而不是暴露一个 “文本输入，文本输出” 的 API，它们暴露的接口是 “聊天消息” 作为输入和输出。

你可以通过将一个或多个消息传递给聊天模型来获取聊天完成。响应将是一条消息。目前在 LangChain 中支持的消息类型有 AIMessage、HumanMessage、SystemMessage 和 ChatMessage--ChatMessage 接受一个任意的角色参数。大多数情况下，你只会处理 HumanMessage、AIMessage 和 SystemMessage。

```python
from langchain.chat_models import ChatOpenAI
from langchain.schema import (
    AIMessage,
    HumanMessage,
    SystemMessage
)

chat = ChatOpenAI(temperature=0)
chat.predict_messages([HumanMessage(content=“Translate this sentence from English to French. I love programming.”)])
# >> AIMessage(content=“J'aime programmer.”, additional_kwargs={})
```

理解聊天模型与普通 LLM 的区别是有用的，但往往能够将它们视为相同是方便的。LangChain 通过暴露一个接口，你可以通过这个接口与聊天模型交互，就像与普通的 LLM 一样。你可以通过 predict 接口访问这个功能。

```python
chat.predict(“Translate this sentence from English to French. I love programming.”)
# >> J'aime programmer
```